{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uJHo_GbVd-cd"
   },
   "source": [
    "# Music generation with LSTM in Keras\n",
    "In this notebook, we will **generate some piano compositions** using a Long Short-Term Memory (LSTM) network. We will use some piano compositions from Chopin to be able to train our network.  We will feed the network with **MIDI files**. These files are not audio files. They contain all the information, notes, chords, etc about a music composition, but they don't contain audio. Our network will be able to generate new MIDI files.\n",
    "\n",
    "\n",
    "This notebook was created to be used in **Google Colaboratory**, so there are some lines of code specially dedicated to upload our files to Google Colab or to obtain some outputs  from the LSTM network in Google Drive. If you are not running this notebook on Google, set **run_on_colab** to false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9FgsdSbLYJ37"
   },
   "outputs": [],
   "source": [
    "# Set to false if you are not running\n",
    "# this notebook in Google Colaboratory\n",
    "run_on_colab = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DDOBVWULXfpz"
   },
   "source": [
    "\n",
    "**This notebook was inspired (and almost all the code comes from it) by [towardsdatascience](https://towardsdatascience.com/how-to-generate-music-using-a-lstm-neural-network-in-keras-68786834d4c5)**\n",
    "\n",
    "**Files used in this notebook can be found at [github](https://github.com/unmonoqueteclea/DeepLearning-Notebooks/tree/master/LSTM-Music-Generation)**\n",
    "\n",
    "## LSTM networks\n",
    "Long Short-Term Memory networks are one type of **Recurrent Neural Network (RNN)**. \n",
    "They are networks whose output depends on the previous ones. This loop behaviour makes them the perfect option to work with sequences and lists. \n",
    "\n",
    "If you want to know more about this type of netwrks read [this amazing post](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) \n",
    "\n",
    "As we can see in [this very famous post](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) , a recurrent neural network can be thought of as multiple copies of the same network, each passing a message to a successor.\n",
    "\n",
    "\n",
    "![RNN vs DNN t](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The problem comes when they have to deal with **long-term dependencies**. Although, teorically, they would be able to handle this dependencies, some researchers have found some pretty fundamental reasons why it might be difficult. LSTM are a special kind of RNN, capable of learning long-term dependencies. Actually, they are designed to remember information for long periods\n",
    "\n",
    "\n",
    "## Music generation\n",
    "To train our model, we will use [MIDI files](https://en.wikipedia.org/wiki/MIDI).\n",
    "MIDI files contain **information about a music composition**, not the music itself. They contain information about notation, pitch, velocity, vibrato, panning, and clock signals (which set tempo).\n",
    "\n",
    "There are some synthesizers which are able to transform this composition information into a real audio track. Our model will learn from these **MIDI files** and will be able to generate new ones. \n",
    "\n",
    "![MIDI file visualization](http://i1-win.softpedia-static.com/screenshots/Speedy-MIDI_1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8Rn0ZBBI1OT8"
   },
   "source": [
    "## Google drive configuration  (only Colab)\n",
    "This will let us use our own **Google Drive** account to store files that can be used inside the Jupyter notebook. When you execute this cell, you will be prompted to visit a web, allow Google Colab to access to your Google Drive account, and copy the authorization code into the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "NP-dwUiX1OEb",
    "outputId": "18f88de4-0bed-4d6d-ec25-30e173df9899"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "if(run_on_colab):\n",
    "  from google.colab import drive\n",
    "  # This will prompt for authorization.\n",
    "  drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q4cCdRCwePQW"
   },
   "source": [
    "## Packages and data\n",
    "Instead of using raw MIDI files, we will process them to obtain only the information we need and discard the rest. \n",
    "\n",
    "That's why we will use [**music21 package**](http://web.mit.edu/music21/). This package contains a set of tools that let us work with MIDI files easily. \n",
    "It creates its own representation of a MIDI file, with different **Note** of **Chord** objects representing all the music inside a MIDI file. It's a representation easier to read than the MIDI one, so it will help our network to *understand* music and be able to create new compositions.\n",
    "\n",
    "Let's install it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Wo4ARjCTdzA0",
    "outputId": "8d561d9f-e591-414c-fbd8-9e59a5338674"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: music21 in /usr/local/lib/python3.6/dist-packages (5.5.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install music21;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i0JdZncylpx-"
   },
   "source": [
    "### Original MIDI files\n",
    " We have obtained  **MIDI files** from [piano-midi.de](http://www.piano-midi.de/midis/format0/). \n",
    " \n",
    " We have downloaded all the MIDI files from Chopin. 50 files, that will be enough to train the network. \n",
    " \n",
    " Feel free to train the network with other authors.\n",
    " \n",
    " If you are note using Google Colaboratory, be sure that you have a **midi_files.zip** file inside your working directory.\n",
    " \n",
    " **FILE DOWNLOAD: https://github.com/unmonoqueteclea/DeepLearning-Notebooks/tree/master/LSTM-Music-Generation**\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "colab_type": "code",
    "id": "IlcAs_63gupm",
    "outputId": "5e40a2bd-ae2d-409a-a06b-a9cf9cd4e9f0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-ec4174ff-da0a-4ef5-ab8a-ca6bfead4217\" name=\"files[]\" multiple disabled />\n",
       "     <output id=\"result-ec4174ff-da0a-4ef5-ab8a-ca6bfead4217\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving midi_files.zip to midi_files.zip\n"
     ]
    }
   ],
   "source": [
    "if(run_on_colab):\n",
    "  from google.colab import files\n",
    "  files.upload()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "i6rzo2WipNDv",
    "outputId": "658f441d-0587-4e33-d8c4-1896be89ad7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  midi_files.zip\n",
      "   creating: midi_files/\n",
      "   creating: midi_files/chopin/\n",
      "  inflating: midi_files/chopin/chpn-p15_format0.mid  \n",
      "   creating: __MACOSX/\n",
      "   creating: __MACOSX/midi_files/\n",
      "   creating: __MACOSX/midi_files/chopin/\n",
      "  inflating: __MACOSX/midi_files/chopin/._chpn-p15_format0.mid  \n",
      "  inflating: midi_files/chopin/chpn_op35_1_format0.mid  \n",
      "  inflating: __MACOSX/midi_files/chopin/._chpn_op35_1_format0.mid  \n",
      "  inflating: midi_files/chopin/chpn-p5_format0.mid  \n",
      "  inflating: __MACOSX/midi_files/chopin/._chpn-p5_format0.mid  \n",
      "  inflating: midi_files/chopin/chpn-p19_format0.mid  \n",
      "  inflating: __MACOSX/midi_files/chopin/._chpn-p19_format0.mid  \n",
      "  inflating: midi_files/chopin/.DS_Store  \n",
      "  inflating: __MACOSX/midi_files/chopin/._.DS_Store  \n",
      "  inflating: midi_files/chopin/chpn_op10_e12_format0.mid  \n",
      "  inflating: __MACOSX/midi_files/chopin/._chpn_op10_e12_format0.mid  \n",
      "  inflating: midi_files/chopin/chpn-p9_format0.mid  \n",
      "  inflating: __MACOSX/midi_files/chopin/._chpn-p9_format0.mid  \n",
      "  inflating: midi_files/chopin/chpn_op25_e2_format0.mid  \n",
      "  inflating: __MACOSX/midi_files/chopin/._chpn_op25_e2_format0.mid  \n",
      "  inflating: midi_files/chopin/chpn-p10_format0.mid  \n",
      "  inflating: __MACOSX/midi_files/chopin/._chpn-p10_format0.mid  \n",
      "  inflating: midi_files/chopin/chpn_op35_4_format0.mid  \n",
      "  inflating: __MACOSX/midi_files/chopin/._chpn_op35_4_format0.mid  \n",
      "  inflating: midi_files/chopin/chpn-p22_format0.mid  \n",
      "  inflating: __MACOSX/midi_files/chopin/._chpn-p22_format0.mid  \n",
      "  inflating: midi_files/chopin/chp_op18_format0.mid  \n",
      "  inflating: __MACOSX/midi_files/chopin/._chp_op18_format0.mid  \n",
      "  inflating: midi_files/chopin/chpn-p21_format0.mid  \n",
      "  inflating: __MACOSX/midi_files/chopin/._chpn-p21_format0.mid  \n",
      "  inflating: midi_files/chopin/chpn-p13_format0.mid  \n",
      "  inflating: __MACOSX/midi_files/chopin/._chpn-p13_format0.mid  \n",
      "  inflating: midi_files/chopin/chpn_op25_e1_format0.mid  \n",
      "  inflating: __MACOSX/midi_files/chopin/._chpn_op25_e1_format0.mid  \n",
      "  inflating: midi_files/chopin/chpn-p3_format0.mid  \n",
      "  inflating: __MACOSX/midi_files/chopin/._chpn-p3_format0.mid  \n",
      "  inflating: midi_files/chopin/chpn-p11-format0.mid  \n",
      "  inflating: __MACOSX/midi_files/chopin/._chpn-p11-format0.mid  \n",
      "  inflating: midi_files/chopin/chpn-p6_format0.mid  \n",
      "  inflating: __MACOSX/midi_files/chopin/._chpn-p6_format0.mid  \n",
      "  inflating: midi_files/chopin/chpn_op66_format0.mid  \n",
      "  inflating: __MACOSX/midi_files/chopin/._chpn_op66_format0.mid  \n",
      "  inflating: midi_files/chopin/chpn_op10_e01_format0.mid  \n",
      "  inflating: __MACOSX/midi_files/chopin/._chpn_op10_e01_format0.mid  \n",
      "  inflating: midi_files/chopin/chpn_op35_2_format0.mid  \n",
      "  inflating: __MACOSX/midi_files/chopin/._chpn_op35_2_format0.mid  \n",
      "  inflating: midi_files/chopin/chpn_op25_e4_format0.mid  \n",
      "  inflating: __MACOSX/midi_files/chopin/._chpn_op25_e4_format0.mid  \n",
      "  inflating: midi_files/chopin/chpn-p16_format0.mid  \n",
      "  inflating: __MACOSX/midi_files/chopin/._chpn-p16_format0.mid  \n",
      "  inflating: midi_files/chopin/chpn_op25_e11_format0.mid  \n",
      "  inflating: __MACOSX/midi_files/chopin/._chpn_op25_e11_format0.mid  \n",
      "  inflating: midi_files/chopin/chpn-p23_format0.mid  \n",
      "  inflating: __MACOSX/midi_files/chopin/._chpn-p23_format0.mid  \n",
      "  inflating: midi_files/chopin/chpn_op25_e3_format0.mid  \n",
      "  inflating: __MACOSX/midi_files/chopin/._chpn_op25_e3_format0.mid  \n",
      "  inflating: midi_files/chopin/chpn-p8_format0.mid  \n",
      "  inflating: __MACOSX/midi_files/chopin/._chpn-p8_format0.mid  \n",
      "  inflating: midi_files/chopin/chpn-p11_format0.mid  \n",
      "  inflating: __MACOSX/midi_files/chopin/._chpn-p11_format0.mid  \n",
      "  inflating: midi_files/chopin/chpn-e11_format0.mid  \n",
      "  inflating: __MACOSX/midi_files/chopin/._chpn-e11_format0.mid  \n",
      "  inflating: midi_files/chopin/chpn_op23_format0.mid  \n",
      "  inflating: __MACOSX/midi_files/chopin/._chpn_op23_format0.mid  \n",
      "  inflating: midi_files/chopin/chpn_op27_2_format0.mid  \n",
      "  inflating: __MACOSX/midi_files/chopin/._chpn_op27_2_format0.mid  \n",
      "  inflating: midi_files/chopin/chpn_op53_format0.mid  \n",
      "  inflating: __MACOSX/midi_files/chopin/._chpn_op53_format0.mid  \n",
      "  inflating: midi_files/chopin/chpn-p1_format0.mid  \n",
      "  inflating: __MACOSX/midi_files/chopin/._chpn-p1_format0.mid  \n",
      "  inflating: midi_files/chopin/chpn-p18_format0.mid  \n",
      "  inflating: __MACOSX/midi_files/chopin/._chpn-p18_format0.mid  \n",
      "  inflating: midi_files/chopin/chpn_op33_4_format.mid  \n",
      "  inflating: __MACOSX/midi_files/chopin/._chpn_op33_4_format.mid  \n",
      "  inflating: midi_files/chopin/chpn_op33_2_format0.mid  \n",
      "  inflating: __MACOSX/midi_files/chopin/._chpn_op33_2_format0.mid  \n",
      "  inflating: midi_files/chopin/chpn-p4_format0.mid  \n",
      "  inflating: __MACOSX/midi_files/chopin/._chpn-p4_format0.mid  \n",
      "  inflating: midi_files/chopin/chpn_op7_1_format0.mid  \n",
      "  inflating: __MACOSX/midi_files/chopin/._chpn_op7_1_format0.mid  \n",
      "  inflating: midi_files/chopin/chpn-p14_format0.mid  \n",
      "  inflating: __MACOSX/midi_files/chopin/._chpn-p14_format0.mid  \n",
      "  inflating: midi_files/chopin/chpn-e01_format0.mid  \n",
      "  inflating: __MACOSX/midi_files/chopin/._chpn-e01_format0.mid  \n",
      "  inflating: midi_files/chopin/chpn-p17_format0.mid  \n",
      "  inflating: __MACOSX/midi_files/chopin/._chpn-p17_format0.mid  \n",
      "  inflating: midi_files/chopin/chpn_op35_3_format0.mid  \n",
      "  inflating: __MACOSX/midi_files/chopin/._chpn_op35_3_format0.mid  \n",
      "  inflating: midi_files/chopin/chp_op31_format0.mid  \n",
      "  inflating: __MACOSX/midi_files/chopin/._chp_op31_format0.mid  \n",
      "  inflating: midi_files/chopin/chpn_op7_2_format0.mid  \n",
      "  inflating: __MACOSX/midi_files/chopin/._chpn_op7_2_format0.mid  \n",
      "  inflating: midi_files/chopin/chpn-p7_format0.mid  \n",
      "  inflating: __MACOSX/midi_files/chopin/._chpn-p7_format0.mid  \n",
      "  inflating: midi_files/chopin/chpn-p2_format0.mid  \n",
      "  inflating: __MACOSX/midi_files/chopin/._chpn-p2_format0.mid  \n",
      "  inflating: midi_files/chopin/chpn-e12_format0.mid  \n",
      "  inflating: __MACOSX/midi_files/chopin/._chpn-e12_format0.mid  \n",
      "  inflating: midi_files/chopin/chpn_op27_1_format0.mid  \n",
      "  inflating: __MACOSX/midi_files/chopin/._chpn_op27_1_format0.mid  \n",
      "  inflating: midi_files/chopin/chpn_op33_4_format0.mid  \n",
      "  inflating: __MACOSX/midi_files/chopin/._chpn_op33_4_format0.mid  \n",
      "  inflating: midi_files/chopin/chpn-p12_format0.mid  \n",
      "  inflating: __MACOSX/midi_files/chopin/._chpn-p12_format0.mid  \n",
      "  inflating: midi_files/chopin/chpn-p20_format0.mid  \n",
      "  inflating: __MACOSX/midi_files/chopin/._chpn-p20_format0.mid  \n",
      "  inflating: midi_files/chopin/chpn_op10_e05_format0.mid  \n",
      "  inflating: __MACOSX/midi_files/chopin/._chpn_op10_e05_format0.mid  \n",
      "  inflating: midi_files/.DS_Store    \n",
      "  inflating: __MACOSX/midi_files/._.DS_Store  \n"
     ]
    }
   ],
   "source": [
    "!unzip midi_files.zip;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VfXedNx5paKS"
   },
   "source": [
    "## Processing data\n",
    "\n",
    "Let's process the files, and load them into **music21**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "bNID-MfGglBp",
    "outputId": "2181e42e-d160-46a3-e3ac-da548a2cb8e9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Importing dependencies\n",
    "import glob\n",
    "import pickle\n",
    "import numpy\n",
    "from music21 import converter, instrument, note, chord, stream\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM, Activation\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UvQHUqw0t8MM"
   },
   "source": [
    "Let's see how **music21** represents music.\n",
    "As we can see below, we have, two different kind of elements:\n",
    "- **Notes**\n",
    "- **Chords**\n",
    "\n",
    "We also have the time offset of each element. This is the time when the note or chord must be played."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "yxLB-WnEuBlR",
    "outputId": "a5c310b5-5b6d-43cf-a61a-750e2bc2f120",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<music21.chord.Chord B3 E2> 0.0\n",
      "<music21.chord.Chord E3 G#3> 0.0\n",
      "<music21.note.Note B> 1/3\n",
      "<music21.chord.Chord E3 G#3> 2/3\n",
      "<music21.note.Note B> 1.0\n",
      "<music21.chord.Chord E-3 F#3> 1.0\n",
      "<music21.note.Note B> 1.0\n",
      "<music21.note.Note B> 4/3\n",
      "<music21.chord.Chord E-3 F#3> 5/3\n",
      "<music21.note.Note B> 1.75\n"
     ]
    }
   ],
   "source": [
    "file = \"midi_files/chopin/chpn-p9_format0.mid\"\n",
    "midi = converter.parse(file)\n",
    "notes_to_parse = midi.flat.notes\n",
    "for element in notes_to_parse[:10]:\n",
    "  print(element, element.offset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_vrSUm7Jgok_"
   },
   "source": [
    "We will process all MIDI files obtaining data from each note of chord.\n",
    "\n",
    "- If we  process a **note**, we will store in the list a string representing the pitch (the note name) and the octave.\n",
    "\n",
    "- If we process a **chord** (Remember that chords are set of notes that are played at the same time) we will store a different type of string with numbers separated by dots. Each number represents the pitch of a chord note. \n",
    "\n",
    "As you can see, **we are not considering yet time offsets of each element**. In this first version, we won't consider them, so all the notes and chords will have the same duration. Maybe, in the future, we will consider them.\n",
    "\n",
    "We are creating a big list with all the elements of all the compositions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "UgaL_DwL9V0P",
    "outputId": "54697ff2-1491-48c4-ff68-a95e91de35df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Parsing file  50   midi_files/chopin/chpn_op10_e05_format0.mid"
     ]
    }
   ],
   "source": [
    "notes = []\n",
    "for i,file in enumerate(glob.glob(\"midi_files/chopin/*.mid\")):\n",
    "  midi = converter.parse(file)\n",
    "  print('\\r', 'Parsing file ', i, \" \",file, end='')\n",
    "  notes_to_parse = None\n",
    "  try: # file has instrument parts\n",
    "    s2 = instrument.partitionByInstrument(midi)\n",
    "    notes_to_parse = s2.parts[0].recurse() \n",
    "  except: # file has notes in a flat structure\n",
    "    notes_to_parse = midi.flat.notes\n",
    "  for element in notes_to_parse:\n",
    "    if isinstance(element, note.Note):\n",
    "      notes.append(str(element.pitch))\n",
    "    elif isinstance(element, chord.Chord):\n",
    "      notes.append('.'.join(str(n) for n in element.normalOrder))\n",
    "with open('notes', 'wb') as filepath:\n",
    "  pickle.dump(notes, filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gHrD-1RijteB"
   },
   "source": [
    "We obtain the number of different notes in our dataset, because this will be the **number of possible output classes**  of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "7Lho6SJW8HP3",
    "outputId": "496ae326-cdcd-4d59-8ea7-d20a328a19d1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "456"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count different possible outputs\n",
    "n_vocab = (len(set(notes)))\n",
    "n_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U5lUC-oTj41-"
   },
   "source": [
    "Now, there is some **data processing** that we have to do:\n",
    "\n",
    "- We will map each pitch or chord to an integer\n",
    "- We will create pairs of input sequences and its corresponding output note\n",
    "\n",
    "We can try different **sequence_length** to obtain different results. In this first version, we will use a sequence_length of 100.\n",
    "\n",
    "The network will made its prediction of the next note (or chord), based on the previous *sequence_length* notes (or chords). \n",
    "\n",
    "![Sequence learning](https://unmonoqueteclea.github.io/assets/images/inputoutputsequences.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K7ewLUx0-vJw"
   },
   "outputs": [],
   "source": [
    "sequence_length = 100\n",
    "# get all pitch names\n",
    "pitchnames = sorted(set(item for item in notes))\n",
    "# create a dictionary to map pitches to integers\n",
    "note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n",
    "network_input = []\n",
    "network_output = []\n",
    "# create input sequences and the corresponding outputs\n",
    "for i in range(0, len(notes) - sequence_length, 1):\n",
    "  sequence_in = notes[i:i + sequence_length] # Size sequence_length\n",
    "  sequence_out = notes[i + sequence_length]  # Size 1\n",
    "  # Map pitches of sequence_in to integers\n",
    "  network_input.append([note_to_int[char] for char in sequence_in])\n",
    "  # Map integer of sequence_out to an integer\n",
    "  network_output.append(note_to_int[sequence_out])\n",
    "n_patterns = len(network_input)\n",
    "# reshape the input into a format compatible with LSTM layers\n",
    "network_input = numpy.reshape(network_input, (n_patterns, sequence_length, 1))\n",
    "# normalize input\n",
    "network_input = network_input / float(n_vocab)\n",
    "network_output = np_utils.to_categorical(network_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cvkOsHQN1P51"
   },
   "source": [
    "Let's see the new metwork_input size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "bppzcWdM1Sk-",
    "outputId": "353d2194-ae89-4140-ac2e-62a9dedb70a9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55535, 100, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network_input.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AfzYkhxEuY8D"
   },
   "source": [
    "## Creating model\n",
    "\n",
    "Let's create the network. We will create a network with 9 layers (3 of them **LSTM layers**).\n",
    "\n",
    "For regularization, we will also add 2 **Dropout** layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IH2fW6Ot-vN-"
   },
   "outputs": [],
   "source": [
    "def create_network(network_input, n_vocab):\n",
    "    \"\"\" create the structure of the neural network \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(\n",
    "        512,\n",
    "        input_shape=(network_input.shape[1], network_input.shape[2]),\n",
    "        return_sequences=True\n",
    "    ))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(LSTM(512, return_sequences=True))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(LSTM(512))\n",
    "    model.add(Dense(256))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(n_vocab))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 734
    },
    "colab_type": "code",
    "id": "yO5HenQR1zwr",
    "outputId": "d4b7d8ec-a3b6-4a7f-cd99-7b5373e34c65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100, 512)          1052672   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100, 512)          0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100, 512)          2099200   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100, 512)          0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 512)               2099200   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 456)               117192    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 456)               0         \n",
      "=================================================================\n",
      "Total params: 5,499,592\n",
      "Trainable params: 5,499,592\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_network(network_input,n_vocab)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NhWEyBhz3WSn"
   },
   "source": [
    "In case we want to use previously trained weights, to continue the training in the point we left it, we should load them into the model. \n",
    "\n",
    "This is very useful in Google Colaboratory, that usually kills the virtual machine that is executing the Jupyter notework after a certime amount of time. If this happens to you, you should have to look for the last weights file in your configured Drive account and use it to train the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EvkVdhBW-voq"
   },
   "outputs": [],
   "source": [
    "# In case we want to use previously trained weights\n",
    "weights = \"\"\n",
    "if(len(weights)>0): model.load_weights(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AC2RDfrvmq6b"
   },
   "source": [
    "We will use **ModelCheckpoint**.\n",
    "\n",
    "ModelCheckpoint will save our weights in a file after each epoch.\n",
    "\n",
    "This way, we can start execution where we left if the training stops.\n",
    "\n",
    "You can train as many epochs as you want. I have checked that, with **75 epochs**, the network is able to compose new interesting music. You can do this 75 epochs using Google Colab GPU in about 13 hours. As you can see, you don't need an extremly low loss, 0.7 or 0.8 is fine.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 446
    },
    "colab_type": "code",
    "id": "tpv-8gT3-vrM",
    "outputId": "31a5fe79-ca0e-437c-85f1-a31d05020965"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/75\n",
      "41792/55535 [=====================>........] - ETA: 2:34 - loss: 4.9710"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-4d8ff04c3a05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mcallbacks_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m model.fit(network_input, network_output, epochs=75, batch_size=64, \n\u001b[0;32m----> 8\u001b[0;31m           callbacks=callbacks_list)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1176\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    202\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "filepath = \"/content/drive/My Drive/{epoch:02d}-{loss:.4f}.h5\"\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss',verbose=0,\n",
    "                             save_best_only=True,mode='min')\n",
    "\n",
    "callbacks_list = [checkpoint]\n",
    "model.fit(network_input, network_output, epochs=75, batch_size=64, \n",
    "          callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qUIMPOdbwJhT"
   },
   "source": [
    "## Music generation\n",
    "\n",
    "Let's compose music!\n",
    "We have renamed our last weights file as **final-weights.h5**.\n",
    "\n",
    " **FILE DOWNLOAD: https://github.com/unmonoqueteclea/DeepLearning-Notebooks/tree/master/LSTM-Music-Generation**\n",
    "\n",
    "There can be generated songs that sound awful, but try to execute the generation\n",
    "process several times and you will get interesting results.\n",
    "\n",
    "![Music Generation](https://unmonoqueteclea.github.io/assets/images/lstm.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C5FHJ6sq8HX7"
   },
   "outputs": [],
   "source": [
    "# In case we want to use other previously trained weights\n",
    "weights = \"/content/final-weights.h5\"\n",
    "if(len(weights)>0): model.load_weights(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qAdOj4d0ExMq"
   },
   "outputs": [],
   "source": [
    "# Generate network input again\n",
    "network_input = []\n",
    "output = []\n",
    "for i in range(0, len(notes) - sequence_length, 1):\n",
    "  sequence_in = notes[i:i + sequence_length]\n",
    "  sequence_out = notes[i + sequence_length]\n",
    "  network_input.append([note_to_int[char] for char in sequence_in])\n",
    "  output.append(note_to_int[sequence_out])\n",
    "n_patterns = len(network_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HjlwzJQv6OKc"
   },
   "source": [
    "The workflow now is:\n",
    "\n",
    "\n",
    "1.   Pick a **seed sequence** randomly from your list of inputs (*pattern* variable)\n",
    "2.   Pass it as input for your model to generate a new element (note or chord)\n",
    "3.   Add the new element to your final song and to your *pattern* list\n",
    "4.   Remove the first item from *pattern*\n",
    "5.   Go to step 2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "F2fkoEiw8HaI",
    "outputId": "89c43a41-da23-4959-dd0a-7f4565c45235"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Predicted  499   2.5.8.10"
     ]
    }
   ],
   "source": [
    "\"\"\" Generate notes from the neural network based on a sequence of notes \"\"\"\n",
    "# pick a random sequence from the input as a starting point for the prediction\n",
    "start = numpy.random.randint(0, len(network_input)-1)\n",
    "int_to_note = dict((number, note) for number, note in enumerate(pitchnames))\n",
    "pattern = network_input[start]\n",
    "prediction_output = []\n",
    "# generate 500 notes\n",
    "for i,note_index in enumerate(range(500)):\n",
    "  prediction_input = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "  prediction_input = prediction_input / float(n_vocab)\n",
    "  prediction = model.predict(prediction_input, verbose=0)\n",
    "  index = numpy.argmax(prediction)\n",
    "  result = int_to_note[index]\n",
    "  print('\\r', 'Predicted ', i, \" \",result, end='')\n",
    "  prediction_output.append(result)\n",
    "  pattern.append(index)\n",
    "  pattern = pattern[1:len(pattern)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RpOqIhgzEAeX"
   },
   "source": [
    "The last step is creating a MIDI file from the predictions.\n",
    "\n",
    "**music21** will help us again for this task. We should create a **Stream** and add to it the predicted notes and chords.\n",
    "\n",
    "We are adding an offset of 0.5 between elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "pub0aRJU8Hcu",
    "outputId": "f78f8a61-62f0-48ca-c588-59cd0c1ce9f3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test_output.mid'"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "offset = 0\n",
    "output_notes = []\n",
    "# create note and chord objects based on the values generated by the model\n",
    "for pattern in prediction_output:\n",
    "    # pattern is a chord\n",
    "    if ('.' in pattern) or pattern.isdigit():\n",
    "        notes_in_chord = pattern.split('.')\n",
    "        notes = []\n",
    "        for current_note in notes_in_chord:\n",
    "            new_note = note.Note(int(current_note))\n",
    "            new_note.storedInstrument = instrument.Piano()\n",
    "            notes.append(new_note)\n",
    "        new_chord = chord.Chord(notes)\n",
    "        new_chord.offset = offset\n",
    "        output_notes.append(new_chord)\n",
    "    # pattern is a note\n",
    "    else:\n",
    "        new_note = note.Note(pattern)\n",
    "        new_note.offset = offset\n",
    "        new_note.storedInstrument = instrument.Piano()\n",
    "        output_notes.append(new_note)\n",
    "\n",
    "    # increase offset each iteration so that notes do not stack\n",
    "    offset += 0.5\n",
    "\n",
    "midi_stream = stream.Stream(output_notes)\n",
    "midi_stream.write('midi', fp='test_output.mid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6_MWlcL4QkjB"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Orginal_Copy_of_Music_Generation_LSTM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
