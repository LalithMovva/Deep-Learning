{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalError",
     "evalue": "Job \"\" was not defined in cluster",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-f78272fab1c3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m     \u001b[0mFLAGS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munparsed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse_known_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 197\u001b[1;33m     \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0munparsed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(main, argv)\u001b[0m\n\u001b[0;32m    123\u001b[0m   \u001b[1;31m# Call the main function, passing through any arguments\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m   \u001b[1;31m# to the final program.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 125\u001b[1;33m   \u001b[0m_sys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-f78272fab1c3>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(_)\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mcluster\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mjob_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         task_index=FLAGS.task_index)\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;31m# config\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\server_lib.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, server_or_cluster_def, job_name, task_index, protocol, config, start)\u001b[0m\n\u001b[0;32m    146\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m       self._server = pywrap_tensorflow.PyServer_New(\n\u001b[1;32m--> 148\u001b[1;33m           self._server_def.SerializeToString(), status)\n\u001b[0m\u001b[0;32m    149\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[0;32m    526\u001b[0m             \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    527\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 528\u001b[1;33m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[0;32m    529\u001b[0m     \u001b[1;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m     \u001b[1;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInternalError\u001b[0m: Job \"\" was not defined in cluster"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import argparse\n",
    "import sys\n",
    "import time\n",
    "\n",
    "FLAGS = None\n",
    "\n",
    "def main(_):\n",
    "    \n",
    "    ps_hosts = FLAGS.ps_hosts.split(',')\n",
    "    worker_hosts = FLAGS.worker_hosts.split(',')\n",
    "\n",
    "    cluster = tf.train.ClusterSpec({\"ps\":ps_hosts, \"worker\":worker_hosts})\n",
    "\n",
    "    # start a server for a specific task\n",
    "    server = tf.train.Server(\n",
    "        cluster,\n",
    "        job_name=FLAGS.job_name,\n",
    "        task_index=FLAGS.task_index)\n",
    "\n",
    "    # config\n",
    "    batch_size = 100\n",
    "    learning_rate = 0.0005\n",
    "    training_epochs = 20\n",
    "    logs_path = \"/tmp/train_logs\"\n",
    "\n",
    "    # load mnist data set\n",
    "    from tensorflow.examples.tutorials.mnist import input_data\n",
    "    # mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "\n",
    "    if FLAGS.job_name == \"ps\":\n",
    "        server.join()\n",
    "    elif FLAGS.job_name == \"worker\":\n",
    "\n",
    "        # Between-graph replication\n",
    "        with tf.device(tf.train.replica_device_setter(\n",
    "            worker_device=\"/job:worker/task:%d\" % FLAGS.task_index,\n",
    "            cluster=cluster)):\n",
    "\n",
    "            mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "\n",
    "            # count the number of updates\n",
    "            global_step = tf.get_variable(\n",
    "                'global_step',\n",
    "                [],\n",
    "                initializer = tf.constant_initializer(0),\n",
    "                trainable = False)\n",
    "\n",
    "            # input images\n",
    "            with tf.name_scope('input'):\n",
    "              # None -> batch size can be any size, 784 -> flattened mnist image\n",
    "              x = tf.placeholder(tf.float32, shape=[None, 784], name=\"x-input\")\n",
    "              # target 10 output classes\n",
    "              y_ = tf.placeholder(tf.float32, shape=[None, 10], name=\"y-input\")\n",
    "\n",
    "            # model parameters will change during training so we use tf.Variable\n",
    "            tf.set_random_seed(1)\n",
    "            with tf.name_scope(\"weights\"):\n",
    "                W1 = tf.Variable(tf.random_normal([784, 100]))\n",
    "                W2 = tf.Variable(tf.random_normal([100, 10]))\n",
    "\n",
    "            # bias\n",
    "            with tf.name_scope(\"biases\"):\n",
    "                b1 = tf.Variable(tf.zeros([100]))\n",
    "                b2 = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "            # implement model\n",
    "            with tf.name_scope(\"softmax\"):\n",
    "                # y is our prediction\n",
    "                z2 = tf.add(tf.matmul(x,W1),b1)\n",
    "                a2 = tf.nn.sigmoid(z2)\n",
    "                z3 = tf.add(tf.matmul(a2,W2),b2)\n",
    "                y  = tf.nn.softmax(z3)\n",
    "\n",
    "            # specify cost function\n",
    "            with tf.name_scope('cross_entropy'):\n",
    "                # this is our cost\n",
    "                cross_entropy = tf.reduce_mean(\n",
    "                    -tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n",
    "\n",
    "            # specify optimizer\n",
    "            with tf.name_scope('train'):\n",
    "                # optimizer is an \"operation\" which we can execute in a session\n",
    "                grad_op = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "                '''\n",
    "                rep_op = tf.train.SyncReplicasOptimizer(\n",
    "                    grad_op,\n",
    "                    replicas_to_aggregate=len(workers),\n",
    "                    replica_id=FLAGS.task_index, \n",
    "                    total_num_replicas=len(workers),\n",
    "                    use_locking=True)\n",
    "                train_op = rep_op.minimize(cross_entropy, global_step=global_step)\n",
    "                '''\n",
    "                train_op = grad_op.minimize(cross_entropy, global_step=global_step)\n",
    "                \n",
    "            '''\n",
    "            init_token_op = rep_op.get_init_tokens_op()\n",
    "            chief_queue_runner = rep_op.get_chief_queue_runner()\n",
    "            '''\n",
    "\n",
    "            with tf.name_scope('Accuracy'):\n",
    "                # accuracy\n",
    "                correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "                accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "            # create a summary for our cost and accuracy\n",
    "            tf.summary.scalar(\"cost\", cross_entropy)\n",
    "            tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "            # merge all summaries into a single \"operation\" which we can execute in a session \n",
    "            summary_op = tf.summary.merge_all()\n",
    "            init_op = tf.global_variables_initializer()\n",
    "            print(\"Variables initialized ...\")\n",
    "\n",
    "        sv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0),\n",
    "                                                            global_step=global_step,\n",
    "                                                            init_op=init_op)\n",
    "\n",
    "        begin_time = time.time()\n",
    "        frequency = 100\n",
    "        with sv.prepare_or_wait_for_session(server.target) as sess:\n",
    "            '''\n",
    "            # is chief\n",
    "            if FLAGS.task_index == 0:\n",
    "                sv.start_queue_runners(sess, [chief_queue_runner])\n",
    "                sess.run(init_token_op)\n",
    "            '''\n",
    "            # create log writer object (this will log on every machine)\n",
    "            writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "                    \n",
    "            # perform training cycles\n",
    "            start_time = time.time()\n",
    "            for epoch in range(training_epochs):\n",
    "\n",
    "                # number of batches in one epoch\n",
    "                batch_count = int(mnist.train.num_examples/batch_size)\n",
    "\n",
    "                count = 0\n",
    "                for i in range(batch_count):\n",
    "                    batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "                    \n",
    "                    # perform the operations we defined earlier on batch\n",
    "                    _, cost, summary, step = sess.run(\n",
    "                                                    [train_op, cross_entropy, summary_op, global_step], \n",
    "                                                    feed_dict={x: batch_x, y_: batch_y})\n",
    "                    writer.add_summary(summary, step)\n",
    "\n",
    "                    count += 1\n",
    "                    if count % frequency == 0 or i+1 == batch_count:\n",
    "                        elapsed_time = time.time() - start_time\n",
    "                        start_time = time.time()\n",
    "                        print(\"Step: %d,\" % (step+1), \n",
    "                                    \" Epoch: %2d,\" % (epoch+1), \n",
    "                                    \" Batch: %3d of %3d,\" % (i+1, batch_count), \n",
    "                                    \" Cost: %.4f,\" % cost, \n",
    "                                    \" AvgTime: %3.2fms\" % float(elapsed_time*1000/frequency))\n",
    "                        count = 0\n",
    "\n",
    "\n",
    "            print(\"Test-Accuracy: %2.2f\" % sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))\n",
    "            print(\"Total Time: %3.2fs\" % float(time.time() - begin_time))\n",
    "            print(\"Final Cost: %.4f\" % cost)\n",
    "\n",
    "        sv.stop()\n",
    "        print(\"done\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.register(\"type\", \"bool\", lambda v: v.lower() == \"true\")\n",
    "    parser.add_argument(\n",
    "        \"--ps_hosts\",\n",
    "        type=str,\n",
    "        default=\"\",\n",
    "        help=\"Comma-separated list of hostname:port pairs\"\n",
    "        )\n",
    "    parser.add_argument(\n",
    "        \"--worker_hosts\",\n",
    "        type=str,\n",
    "        default=\"\",\n",
    "        help=\"Comma-separated list of hostname:port pairs\"\n",
    "        )\n",
    "    parser.add_argument(\n",
    "        \"--job_name\",\n",
    "        type=str,\n",
    "        default=\"\",\n",
    "        help=\"One of 'ps', 'worker'\"\n",
    "        )\n",
    "    parser.add_argument(\n",
    "        \"--task_index\",\n",
    "        type=int,\n",
    "        default=0,\n",
    "        help=\"Index of task within the job\"\n",
    "        )\n",
    "\n",
    "    FLAGS, unparsed = parser.parse_known_args()\n",
    "    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
