{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pd0LxpLI2cFF"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import random\n",
    "import tarfile\n",
    "import tempfile\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch                    \n",
    "import torchvision\n",
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IqR-9YxZ3a9S"
   },
   "outputs": [],
   "source": [
    "data_dir = Path.cwd().parent / \"content\" / \"data\"\n",
    "\n",
    "img_tar_path = data_dir / \"img\"\n",
    "train_path = data_dir / \"train.jsonl\"\n",
    "dev_path = data_dir / \"dev_seen.jsonl\"\n",
    "test_path = data_dir / \"test_seen.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sJhgL9Ou3cIE"
   },
   "outputs": [],
   "source": [
    "train_samples_frame = pd.read_json(train_path, lines=True)\n",
    "train_samples_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ujvBjoRG5m2c"
   },
   "outputs": [],
   "source": [
    "train_samples_frame.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cgAdAewq5x2p"
   },
   "outputs": [],
   "source": [
    "train_samples_frame.text.map(\n",
    "    lambda text: len(text.split(\" \"))\n",
    ").describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sZATAQuu6F7q"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "\n",
    "images = [\n",
    "    Image.open(\n",
    "        data_dir / train_samples_frame.loc[i, \"img\"]\n",
    "    ).convert(\"RGB\")\n",
    "    for i in range(5)\n",
    "]\n",
    "\n",
    "for image in images:\n",
    "    print(image.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JTMEF5P465Ow"
   },
   "outputs": [],
   "source": [
    "image_transform = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.Resize(size=(224, 224)),\n",
    "        torchvision.transforms.ToTensor()\n",
    "    ]\n",
    ")\n",
    "\n",
    "# convert the images and prepare for visualization.\n",
    "tensor_img = torch.stack(\n",
    "    [image_transform(image) for image in images]\n",
    ")\n",
    "grid = torchvision.utils.make_grid(tensor_img)\n",
    "\n",
    "# plot\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 5)\n",
    "plt.axis('off')\n",
    "_ = plt.imshow(grid.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "66lBNVED7FMI"
   },
   "outputs": [],
   "source": [
    "class HatefulMemesDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_path,\n",
    "        img_dir,\n",
    "        image_transform,\n",
    "        text_transform,\n",
    "        balance=False,\n",
    "        dev_limit=None,\n",
    "        random_state=0,\n",
    "    ):\n",
    "\n",
    "        self.samples_frame = pd.read_json(data_path, lines=True)\n",
    "        self.dev_limit = dev_limit\n",
    "        if balance:\n",
    "            neg = self.samples_frame[self.samples_frame.label.eq(0)]\n",
    "            pos = self.samples_frame[self.samples_frame.label.eq(1)]\n",
    "            \n",
    "            self.samples_frame = pd.concat([neg.sample(pos.shape[0], random_state=random_state),\n",
    "                                            pos])\n",
    "\n",
    "        if self.dev_limit:\n",
    "            if self.samples_frame.shape[0] > self.dev_limit:\n",
    "                self.samples_frame = self.samples_frame.sample(\n",
    "                    dev_limit, random_state=random_state\n",
    "                )\n",
    "        self.samples_frame = self.samples_frame.reset_index(drop=True)\n",
    "        \n",
    "        self.samples_frame.img = self.samples_frame.apply(\n",
    "            lambda row: (img_dir / row.img), axis=1\n",
    "        )\n",
    "\n",
    "            \n",
    "        self.image_transform = image_transform\n",
    "        self.text_transform = text_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.samples_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_id = self.samples_frame.loc[idx, \"id\"]\n",
    "\n",
    "        image = Image.open(\n",
    "            self.samples_frame.loc[idx, \"img\"]\n",
    "        ).convert(\"RGB\")\n",
    "        image = self.image_transform(image)\n",
    "\n",
    "        text = torch.Tensor(\n",
    "            self.text_transform.get_sentence_vector(\n",
    "                self.samples_frame.loc[idx, \"text\"]\n",
    "            )\n",
    "        ).squeeze()\n",
    "\n",
    "        if \"label\" in self.samples_frame.columns:\n",
    "            label = torch.Tensor(\n",
    "                [self.samples_frame.loc[idx, \"label\"]]\n",
    "            ).long().squeeze()\n",
    "            sample = {\n",
    "                \"id\": img_id, \n",
    "                \"image\": image, \n",
    "                \"text\": text, \n",
    "                \"label\": label\n",
    "            }\n",
    "        else:\n",
    "            sample = {\n",
    "                \"id\": img_id, \n",
    "                \"image\": image, \n",
    "                \"text\": text\n",
    "            }\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dbvp6-PlrlZ7"
   },
   "outputs": [],
   "source": [
    "class LanguageAndVisionConcat(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes,\n",
    "        loss_fn,\n",
    "        language_module,\n",
    "        vision_module,\n",
    "        language_feature_dim,\n",
    "        vision_feature_dim,\n",
    "        fusion_output_size,\n",
    "        dropout_p,\n",
    "        \n",
    "    ):\n",
    "        super(LanguageAndVisionConcat, self).__init__()\n",
    "        self.language_module = language_module\n",
    "        self.vision_module = vision_module\n",
    "        self.fusion = torch.nn.Linear(\n",
    "            in_features=(language_feature_dim + vision_feature_dim), \n",
    "            out_features=fusion_output_size\n",
    "        )\n",
    "        self.fc = torch.nn.Linear(\n",
    "            in_features=fusion_output_size, \n",
    "            out_features=num_classes\n",
    "        )\n",
    "        self.loss_fn = loss_fn\n",
    "        self.dropout = torch.nn.Dropout(dropout_p)\n",
    "        \n",
    "    def forward(self, text, image, label=None):\n",
    "        text_features = torch.nn.functional.relu(\n",
    "            self.language_module(text)\n",
    "        )\n",
    "        image_features = torch.nn.functional.relu(\n",
    "            self.vision_module(image)\n",
    "        )\n",
    "        combined = torch.cat(\n",
    "            [text_features, image_features], dim=1\n",
    "        )\n",
    "        fused = self.dropout(\n",
    "            torch.nn.functional.relu(\n",
    "            self.fusion(combined)\n",
    "            )\n",
    "        )\n",
    "        logits = self.fc(fused)\n",
    "        pred = torch.nn.functional.softmax(logits)\n",
    "        loss = (\n",
    "            self.loss_fn(pred, label) \n",
    "            if label is not None else label\n",
    "        )\n",
    "        return (pred, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YX0wtQODtQgI"
   },
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.metrics import functional as FM\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.getLogger().setLevel(logging.WARNING)\n",
    "\n",
    "\n",
    "class HatefulMemesModel(pl.LightningModule):\n",
    "    def __init__(self, hparams):\n",
    "        for data_key in [\"train_path\", \"dev_path\", \"img_dir\",]:\n",
    "            if data_key not in hparams.keys():\n",
    "                raise KeyError(\n",
    "                    f\"{data_key} is a required hparam in this model\"\n",
    "                )\n",
    "        \n",
    "        super(HatefulMemesModel, self).__init__()\n",
    "        self.hparams = hparams\n",
    "        \n",
    "        self.embedding_dim = self.hparams.get(\"embedding_dim\", 300)\n",
    "        self.language_feature_dim = self.hparams.get(\"language_feature_dim\", 300)\n",
    "        self.vision_feature_dim = self.hparams.get(\"vision_feature_dim\", self.language_feature_dim)\n",
    "        self.output_path = Path(self.hparams.get(\"output_path\", \"model-outputs\"))\n",
    "        self.output_path.mkdir(exist_ok=True)\n",
    "        \n",
    "        self.text_transform = self._build_text_transform()\n",
    "        self.image_transform = self._build_image_transform()\n",
    "        self.train_dataset = self._build_dataset(\"train_path\")\n",
    "        self.dev_dataset = self._build_dataset(\"dev_path\")\n",
    "        \n",
    "        self.model = self._build_model()\n",
    "        self.trainer_params = self._get_trainer_params()\n",
    "    \n",
    "    \n",
    "    def forward(self, text, image, label=None):\n",
    "        return self.model(text, image, label)\n",
    "\n",
    "    def training_step(self, batch, batch_nb):\n",
    "        preds, loss = self.forward(\n",
    "            text=batch[\"text\"], \n",
    "            image=batch[\"image\"], \n",
    "            label=batch[\"label\"]\n",
    "        )\n",
    "        \n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    def validation_step(self, batch, batch_nb):\n",
    "        preds, loss = self.eval().forward(\n",
    "            text=batch[\"text\"], \n",
    "            image=batch[\"image\"], \n",
    "            label=batch[\"label\"]\n",
    "        )\n",
    "\n",
    "        acc = FM.accuracy(preds, batch[\"label\"], num_classes=2)\n",
    "        \n",
    "        return {\"batch_val_loss\": loss, \"batch_val_acc\": acc}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack(\n",
    "            tuple(\n",
    "                output[\"batch_val_loss\"] \n",
    "                for output in outputs\n",
    "            )\n",
    "        ).mean()\n",
    "        \n",
    "        avg_acc = torch.stack(\n",
    "            tuple(\n",
    "                output[\"batch_val_acc\"] \n",
    "                for output in outputs\n",
    "            )\n",
    "        ).mean()\n",
    "\n",
    "        return {\n",
    "            \"val_loss\": avg_loss,\n",
    "            \"progress_bar\":{\"avg_val_loss\": avg_loss, \"avg_val_acc\": avg_acc}\n",
    "        }\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizers = [\n",
    "            torch.optim.AdamW(\n",
    "                self.model.parameters(), \n",
    "                lr=self.hparams.get(\"lr\", 0.001)\n",
    "            )\n",
    "        ]\n",
    "        schedulers = [\n",
    "            torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizers[0]\n",
    "            )\n",
    "        ]\n",
    "        return optimizers, schedulers\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.train_dataset, \n",
    "            shuffle=True, \n",
    "            batch_size=self.hparams.get(\"batch_size\", 4), \n",
    "            num_workers=self.hparams.get(\"num_workers\", 16)\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.dev_dataset, \n",
    "            shuffle=False, \n",
    "            batch_size=self.hparams.get(\"batch_size\", 4), \n",
    "            num_workers=self.hparams.get(\"num_workers\", 16)\n",
    "        )\n",
    "    \n",
    "    \n",
    "    def fit(self):\n",
    "        self._set_seed(self.hparams.get(\"random_state\", 42))\n",
    "        self.trainer = pl.Trainer(**self.trainer_params)\n",
    "        self.trainer.fit(self)\n",
    "        \n",
    "    def _set_seed(self, seed):\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    def _build_text_transform(self):\n",
    "        with tempfile.NamedTemporaryFile() as ft_training_data:\n",
    "            ft_path = Path(ft_training_data.name)\n",
    "            with ft_path.open(\"w\") as ft:\n",
    "                training_data = [\n",
    "                    json.loads(line)[\"text\"] + \"/n\" \n",
    "                    for line in open(\n",
    "                        self.hparams.get(\"train_path\")\n",
    "                    ).read().splitlines()\n",
    "                ]\n",
    "                for line in training_data:\n",
    "                    ft.write(line + \"\\n\")\n",
    "                language_transform = fasttext.train_unsupervised(\n",
    "                    str(ft_path),\n",
    "                    model=self.hparams.get(\"fasttext_model\", \"cbow\"),\n",
    "                    dim=self.embedding_dim\n",
    "                )\n",
    "        return language_transform\n",
    "    \n",
    "    def _build_image_transform(self):\n",
    "        image_dim = self.hparams.get(\"image_dim\", 224)\n",
    "        image_transform = torchvision.transforms.Compose(\n",
    "            [\n",
    "                torchvision.transforms.Resize(\n",
    "                    size=(image_dim, image_dim)\n",
    "                ),        \n",
    "                torchvision.transforms.ToTensor(),\n",
    "                \n",
    "                torchvision.transforms.Normalize(\n",
    "                    mean=(0.485, 0.456, 0.406), \n",
    "                    std=(0.229, 0.224, 0.225)\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        return image_transform\n",
    "\n",
    "    def _build_dataset(self, dataset_key):\n",
    "        return HatefulMemesDataset(\n",
    "            data_path=self.hparams.get(dataset_key, dataset_key),\n",
    "            img_dir=self.hparams.get(\"img_dir\"),\n",
    "            image_transform=self.image_transform,\n",
    "            text_transform=self.text_transform,\n",
    "            dev_limit=(\n",
    "                self.hparams.get(\"dev_limit\", None) \n",
    "                if \"train\" in str(dataset_key) else None\n",
    "            ),\n",
    "            balance=True if \"train\" in str(dataset_key) else False,\n",
    "        )\n",
    "    \n",
    "    def _build_model(self):\n",
    "        \n",
    "        language_module = torch.nn.Linear(\n",
    "                in_features=self.embedding_dim,\n",
    "                out_features=self.language_feature_dim\n",
    "        )\n",
    "        \n",
    "        \n",
    "        vision_module = torchvision.models.resnet152(\n",
    "            pretrained=True\n",
    "        )\n",
    "        vision_module.fc = torch.nn.Linear(\n",
    "                in_features=2048,\n",
    "                out_features=self.vision_feature_dim\n",
    "        )\n",
    "\n",
    "        return LanguageAndVisionConcat(\n",
    "            num_classes=self.hparams.get(\"num_classes\", 2),\n",
    "            loss_fn=torch.nn.CrossEntropyLoss(),\n",
    "            language_module=language_module,\n",
    "            vision_module=vision_module,\n",
    "            language_feature_dim=self.language_feature_dim,\n",
    "            vision_feature_dim=self.vision_feature_dim,\n",
    "            fusion_output_size=self.hparams.get(\n",
    "                \"fusion_output_size\", 512\n",
    "            ),\n",
    "            dropout_p=self.hparams.get(\"dropout_p\", 0.1),\n",
    "        )\n",
    "    \n",
    "    def _get_trainer_params(self):\n",
    "        checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "            filepath=self.output_path,\n",
    "            monitor=self.hparams.get(\n",
    "                \"checkpoint_monitor\", \"avg_val_acc\"\n",
    "            ),\n",
    "            mode=self.hparams.get(\n",
    "                \"checkpoint_monitor_mode\", \"max\"\n",
    "            ),\n",
    "            verbose=self.hparams.get(\"verbose\", True)\n",
    "        )\n",
    "\n",
    "        early_stop_callback = pl.callbacks.EarlyStopping(\n",
    "            monitor=self.hparams.get(\n",
    "                \"early_stop_monitor\", \"avg_val_acc\"\n",
    "            ),\n",
    "            min_delta=self.hparams.get(\n",
    "                \"early_stop_min_delta\", 0.001\n",
    "            ),\n",
    "            patience=self.hparams.get(\n",
    "                \"early_stop_patience\", 5\n",
    "            ),\n",
    "            verbose=self.hparams.get(\"verbose\", True),\n",
    "        )\n",
    "\n",
    "        trainer_params = {\n",
    "            \"checkpoint_callback\": checkpoint_callback,\n",
    "            \"early_stop_callback\": early_stop_callback,\n",
    "            # \"default_save_path\": self.output_path,\n",
    "            \"accumulate_grad_batches\": self.hparams.get(\n",
    "                \"accumulate_grad_batches\", 1\n",
    "            ),\n",
    "            \"gpus\": self.hparams.get(\"n_gpu\", 1),\n",
    "            \"max_epochs\": self.hparams.get(\"max_epochs\", 100),\n",
    "            \"gradient_clip_val\": self.hparams.get(\n",
    "                \"gradient_clip_value\", 1\n",
    "            ),\n",
    "        }\n",
    "        return trainer_params\n",
    "            \n",
    "    @torch.no_grad()\n",
    "    def make_submission_frame(self, test_path):\n",
    "        test_dataset = self._build_dataset(test_path)\n",
    "        submission_frame = pd.DataFrame(\n",
    "            index=test_dataset.samples_frame.id,\n",
    "            columns=[\"proba\", \"label\"]\n",
    "        )\n",
    "        test_dataloader = torch.utils.data.DataLoader(\n",
    "            test_dataset, \n",
    "            shuffle=False, \n",
    "            batch_size=self.hparams.get(\"batch_size\", 4), \n",
    "            num_workers=self.hparams.get(\"num_workers\", 16))\n",
    "        for batch in tqdm(test_dataloader, total=len(test_dataloader)):\n",
    "            preds, _ = self.model.eval().to(\"cpu\")(\n",
    "                batch[\"text\"], batch[\"image\"]\n",
    "            )\n",
    "            submission_frame.loc[batch[\"id\"], \"proba\"] = preds[:, 1]\n",
    "            submission_frame.loc[batch[\"id\"], \"label\"] = preds.argmax(dim=1)\n",
    "        submission_frame.proba = submission_frame.proba.astype(float)\n",
    "        submission_frame.label = submission_frame.label.astype(int)\n",
    "        return submission_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9YULv0-qEO1-"
   },
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    \n",
    "    \"train_path\": train_path,\n",
    "    \"dev_path\": dev_path,\n",
    "    \"img_dir\": data_dir,\n",
    "    \n",
    "    \"embedding_dim\": 150,\n",
    "    \"language_feature_dim\": 300,\n",
    "    \"vision_feature_dim\": 300,\n",
    "    \"fusion_output_size\": 256,\n",
    "    \"output_path\": \"model-outputs\",\n",
    "    \"dev_limit\": None,\n",
    "    \"lr\": 0.0005,\n",
    "    \"max_epochs\": 10,\n",
    "    \"n_gpu\": 1,\n",
    "    \"batch_size\": 4,\n",
    "    \"accumulate_grad_batches\": 16,\n",
    "    \"early_stop_patience\": 3,\n",
    "}\n",
    "\n",
    "hateful_memes_model = HatefulMemesModel(hparams=hparams)\n",
    "hateful_memes_model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Aymg9hzbA_P"
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0B0upKS_EVOo"
   },
   "outputs": [],
   "source": [
    "checkpoints = list(Path(\"model-outputs\").glob(\"*.ckpt\"))\n",
    "assert len(checkpoints) == 1\n",
    "\n",
    "checkpoints"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNF+/72xQwIxUtFlrOjpbaK",
   "collapsed_sections": [
    "BTA0GliX2ue4",
    "X1Yd_i33N8NP",
    "6cH9ILYNN_Dq",
    "rdnpDRFrOC14",
    "oZZdL-AP2yBC",
    "wzr4zSKA7RCH",
    "N8XY0ETV5oT5",
    "Z4p4nEi96Ebx",
    "9za5zpie7CwP",
    "h5MyHXnI7taf",
    "dK40cxVADqhy",
    "gcwtOdPjENb2",
    "Eyfx8b1sETX_"
   ],
   "include_colab_link": true,
   "name": "[GitHub]simple_model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
